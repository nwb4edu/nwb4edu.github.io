{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1655aa86",
   "metadata": {},
   "source": [
    "# Spike Sorting\n",
    "\n",
    "When you record extracellular electrophysiology data, one of the first data processing steps is figuring out which action potentials (or \"spikes\") came from which neurons. The process of doing this is called **spike sorting**.\n",
    "\n",
    "Below, we'll work with [this dataset](https://dandiarchive.org/dandiset/000053/0.210819.0345) from [Lisa Giocomo's lab at Stanford](https://giocomolab.weebly.com/) to demonstrate the simplest form of spike sorting: thresholding, followed by feature extraction.\n",
    "\n",
    "<mark>**Note**:  The code below requires a different dataset than the one we interacted with in the last chapter. Because this dataset contains all of the raw recording data, it is much, much bigger. As a result, the best way to work with it is through the Dandihub. If you're not already running this book on the Dandihub, read [\"Using this Book](https://nwb4edu.github.io/Chapter_01/Using_This_Book.html) for instructions on how to run this on the Dandihub.</mark>\n",
    "\n",
    "## Step 1. Inspect the Data\n",
    "\n",
    "First, we need to find the correct URL for the dataset on the NWB's Amazon S3 storage system. There is a tool to do so within the dandiapi, which we'll use below to get the URL for one session within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a37ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version (0.56.0) of dandi/dandi-cli is available. You are using 0.55.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dandiarchive.s3.amazonaws.com/blobs/d6d/882/d6d88284-983a-42a7-91e8-48e9f59c2881\n"
     ]
    }
   ],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "dandiset_id = '000053' # giocomo data\n",
    "filepath = 'sub-npI5/sub-npI5_ses-20190414_behavior+ecephys.nwb' \n",
    "\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "    \n",
    "print(s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f0e81",
   "metadata": {},
   "source": [
    "Now that we have this path, we can stream the data rather than downloading it. Below, we'll print some useful information about this experiment. We will also access a dataset we haven't interacted with yet: [`ElectricalSeries`](https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html). As the name suggests, this group contains raw electrophysiology data -- exactly what we need to sort! We will assign a portion of this to an object called `ephys_data`.\n",
    "\n",
    "<mark>**Note**: The cell below will take about a minute to run, depending on the speed of your internet connection.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc3e12e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "h5py was built without ROS3 support, can't use ros3 driver",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynwb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NWBHDF5IO\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mNWBHDF5IO\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_namespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mros3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m io:\n\u001b[1;32m      4\u001b[0m     nwbfile \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(nwbfile\u001b[38;5;241m.\u001b[39macquisition[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElectricalSeries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/utils.py:645\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    644\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/pynwb/__init__.py:233\u001b[0m, in \u001b[0;36mNWBHDF5IO.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot load namespaces from file when writing to it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m     tm \u001b[38;5;241m=\u001b[39m get_type_map()\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_namespaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     manager \u001b[38;5;241m=\u001b[39m BuildManager(tm)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# XXX: Leaving this here in case we want to revert to this strategy for\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m#      loading cached namespaces\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# ns_catalog = NamespaceCatalog(NWBGroupSpec, NWBDatasetSpec, NWBNamespace)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# tm.copy_mappers(get_type_map())\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/utils.py:645\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    644\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:149\u001b[0m, in \u001b[0;36mHDF5IO.load_namespaces\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load cached namespaces from a file.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mIf `file` is not supplied, then an :py:class:`h5py.File` object will be opened for the given `path`, the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m:raises ValueError: if both `path` and `file` are supplied but `path` is not the same as the path of `file`.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m namespace_catalog, path, namespaces, file_obj, driver \u001b[38;5;241m=\u001b[39m popargs(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace_catalog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespaces\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs)\n\u001b[0;32m--> 149\u001b[0m open_file_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__resolve_file_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# need to close the file object that we just opened\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m open_file_obj:\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:124\u001b[0m, in \u001b[0;36mHDF5IO.__resolve_file_obj\u001b[0;34m(cls, path, file_obj, driver)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m driver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         file_kwargs\u001b[38;5;241m.\u001b[39mupdate(driver\u001b[38;5;241m=\u001b[39mdriver)\n\u001b[0;32m--> 124\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfile_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_obj\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/h5py/_hl/files.py:518\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: S3 location must begin with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    516\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py was built without ROS3 support, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use ros3 driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m locking \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hdf5_version \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    522\u001b[0m         hdf5_version[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m hdf5_version[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDF5 version >= 1.12.1 or 1.10.x >= 1.10.7 required for file locking options.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: h5py was built without ROS3 support, can't use ros3 driver"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "with NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3') as io:\n",
    "    nwbfile = io.read()\n",
    "    print(nwbfile.acquisition['ElectricalSeries'].data.shape)\n",
    "    sampling_freq = nwbfile.acquisition['ElectricalSeries'].rate # get the sampling frequency in Hz\n",
    "    ephys_data = (nwbfile.acquisition['ElectricalSeries'].data[:3000000, 99])*nwbfile.acquisition['ElectricalSeries'].conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe86543",
   "metadata": {},
   "source": [
    "Before we dive into spike sorting, let's take a look at the data. Below, we'll import a couple additional packages, generate a list of timestamps, and plot it.\n",
    "\n",
    ">**Note**: Need a reminder on how some of the packages or functions below work? Look through the [NumPy](https://nwb4edu.github.io/Chapter_02/NumPy.html) and [Matplotlib](https://nwb4edu.github.io/Chapter_02/Matplotlib.html) review pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate a vector of timestamps\n",
    "timestamps = np.arange(0, len(ephys_data)) * (1.0 / sampling_freq)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,3))\n",
    "plt.plot(timestamps,ephys_data)\n",
    "plt.ylabel('Voltage (V)')\n",
    "plt.xlabel('Seconds (s)')\n",
    "#plt.xlim(1.053,1.055)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5f787",
   "metadata": {},
   "source": [
    "In the data above, there are clear places where the data \"spikes\". These are extracellularly recorded action potentials!\n",
    "\n",
    "## Step 2. Spike Detection\n",
    "\n",
    "One of the most straightforward ways to spike sort is to simply detect these using a threshold. Whenever the signal passes this threshold, we'll clip that out. We could determine a reasonable threshold by eye, but we'll do this mathematically instead, using the standard deviation of the signal. When the signal is five times the standard deviation, that's enough signal to noise that it's likely to be an action potential. We'll calculate that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = np.std(ephys_data)\n",
    "recommended_threshold = -5 * noise_std\n",
    "print('Noise Estimate by Standard Deviation: ', noise_std)\n",
    "print('Recommended Spike Threshold         : ', recommended_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87219f13",
   "metadata": {},
   "source": [
    "Our spike detector needs to take into account that a spike typically comprises multiple samples, so we can't simply take each sample that exceeds the threshold as an individual spike detection. Instead, we'll define a *dead time*: whenever we detect a spike, the next few samples within the dead time won't trigger a spike detection by themselves.\n",
    "\n",
    "In order to generate a more precise idea of where each spike is we will also search for the minimum in each spike by lookiing through the signal for a short period of time after the threshold crossing. The time of this minimum will be the timestamp for each spike.\n",
    "\n",
    "Below, we'll define three helper functions to accomplish this detection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24fe6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_threshold_crossings(signal, fs, threshold, dead_time):\n",
    "    \"\"\"\n",
    "    Detect threshold crossings in a signal with dead time and return them as an array\n",
    "    \n",
    "    The signal transitions from a sample above the threshold to a sample below the threshold for a detection and\n",
    "    the last detection has to be more than dead_time apart from the current one.\n",
    "    \n",
    "    :param signal: The signal as a 1-dimensional numpy array\n",
    "    :param fs: The sampling frequency in Hz\n",
    "    :param threshold: The threshold for the signal\n",
    "    :param dead_time: The dead time in seconds. \n",
    "    \"\"\"\n",
    "    dead_time_idx = dead_time * fs\n",
    "    threshold_crossings = np.diff((signal <= threshold).astype(int) > 0).nonzero()[0]\n",
    "    distance_sufficient = np.insert(np.diff(threshold_crossings) >= dead_time_idx, 0, True)\n",
    "    while not np.all(distance_sufficient):\n",
    "        # repeatedly remove all threshold crossings that violate the dead_time\n",
    "        threshold_crossings = threshold_crossings[distance_sufficient]\n",
    "        distance_sufficient = np.insert(np.diff(threshold_crossings) >= dead_time_idx, 0, True)\n",
    "    return threshold_crossings\n",
    "\n",
    "def get_next_minimum(signal, index, max_samples_to_search):\n",
    "    \"\"\"\n",
    "    Returns the index of the next minimum in the signal after an index\n",
    "    \n",
    "    :param signal: The signal as a 1-dimensional numpy array\n",
    "    :param index: The scalar index \n",
    "    :param max_samples_to_search: The number of samples to search for a minimum after the index\n",
    "    \"\"\"\n",
    "    search_end_idx = min(index + max_samples_to_search, signal.shape[0])\n",
    "    min_idx = np.argmin(signal[index:search_end_idx])\n",
    "    return index + min_idx\n",
    "\n",
    "def align_to_minimum(signal, fs, threshold_crossings, search_range):\n",
    "    \"\"\"\n",
    "    Returns the index of the next negative spike peak for all threshold crossings\n",
    "    \n",
    "    :param signal: The signal as a 1-dimensional numpy array\n",
    "    :param fs: The sampling frequency in Hz\n",
    "    :param threshold_crossings: The array of indices where the signal crossed the detection threshold\n",
    "    :param search_range: The maximum duration in seconds to search for the minimum after each crossing\n",
    "    \"\"\"\n",
    "    search_end = int(search_range*fs)\n",
    "    aligned_spikes = [get_next_minimum(signal, t, search_end) for t in threshold_crossings]\n",
    "    return np.array(aligned_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d7a04",
   "metadata": {},
   "source": [
    "Now, we can use those functions to detect threshold crossings. We'll then plot our original signal with the detected spikes marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7925522",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossings = detect_threshold_crossings(ephys_data, sampling_freq, recommended_threshold, 0.003) # dead time of 3 ms\n",
    "spks = align_to_minimum(ephys_data, sampling_freq, crossings, 0.002) # search range 2 ms\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(20,5))\n",
    "plt.plot(ephys_data)\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Voltage (uV)')\n",
    "plt.plot(spks,[recommended_threshold]*spks.shape[0], 'ro', ms=2)\n",
    "ax.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.xlim([0,len(ephys_data)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bdc57",
   "metadata": {},
   "source": [
    "Finally, we can cut out the waveforms from these detected spikes so that we can look at their shape. To do so, we will cut out a portion of the signal around each spike. Spikes too close to the start or end of the signal that a full cutout is not possible are ignored. \n",
    "\n",
    "The location and shape of a waveform is one of the main pieces of evidence to show that the waveform was recorded from a cell. This will help us determine whether or not there is just one or more neurons recorded on this channel. \n",
    "\n",
    "Below, we will define two helper functions: one to extract the waveforms, and one to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveforms(signal, fs, spikes_idx, pre, post):\n",
    "    \"\"\"\n",
    "    Extract spike waveforms as signal cutouts around each spike index as a spikes x samples numpy array\n",
    "    \n",
    "    :param signal: The signal as a 1-dimensional numpy array\n",
    "    :param fs: The sampling frequency in Hz\n",
    "    :param spikes_idx: The sample index of all spikes as a 1-dim numpy array\n",
    "    :param pre: The duration of the cutout before the spike in seconds\n",
    "    :param post: The duration of the cutout after the spike in seconds\n",
    "    \"\"\"\n",
    "    cutouts = []\n",
    "    pre_idx = int(pre * fs)\n",
    "    post_idx = int(post * fs)\n",
    "    for index in spikes_idx:\n",
    "        if index-pre_idx >= 0 and index+post_idx <= signal.shape[0]:\n",
    "            cutout = signal[(index-pre_idx):(index+post_idx)]\n",
    "            cutouts.append(cutout)\n",
    "    return np.stack(cutouts)\n",
    "\n",
    "def plot_waveforms(cutouts, fs, pre, post, n=100):\n",
    "    \"\"\"\n",
    "    Plot an overlay of spike cutouts\n",
    "    \n",
    "    :param cutouts: A spikes x samples array of cutouts\n",
    "    :param fs: The sampling frequency in Hz\n",
    "    :param pre: The duration of the cutout before the spike in seconds\n",
    "    :param post: The duration of the cutout after the spike in seconds\n",
    "    :param n: The number of cutouts to plot, or None to plot all. Default: 100\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = cutouts.shape[0]\n",
    "    n = min(n, cutouts.shape[0])\n",
    "    time_in_us = np.arange(-pre, post, 1/fs)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    for i in range(n):\n",
    "        plt.plot(time_in_us, cutouts[i,], color='k', linewidth=1, alpha=0.3)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Voltage (V)' )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f068f78",
   "metadata": {},
   "source": [
    "Now, let's extract & plot the waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = 0.001 # 1 ms\n",
    "post= 0.002 # 2 ms\n",
    "waveforms = extract_waveforms(ephys_data, sampling_freq, spks, pre, post)\n",
    "plot_waveforms(waveforms, sampling_freq, pre, post, n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db614c46",
   "metadata": {},
   "source": [
    "Looking at the plot above, would you say there is just one waveform here, or multiple?\n",
    "\n",
    "## Step 3. Feature Extraction\n",
    "\n",
    "Indeed, it looks like there might be two -- one that is very high amplitude, and another that is lower amplitude. We can use **feature extraction** to investigate our waveforms. Below, we'll plot the minimum and maximum voltages in the recorded waveforms to see if there are distinct clusters of waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_voltage = np.amin(waveforms, axis=1)\n",
    "max_voltage = np.amax(waveforms, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(min_voltage, max_voltage,'.')\n",
    "plt.xlabel('Minimum Voltage (V)')\n",
    "plt.ylabel('Maximum Voltage (V)')\n",
    "plt.title('Min/Max Spike Voltages')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f63dab",
   "metadata": {},
   "source": [
    "Looking at the scatterplot above, would you say there is more than one cluster of waveforms?\n",
    "\n",
    "For these two units, separating by minimum & maximum voltage actually works fairly well! However, many action potentials have shapes that cannot be easily summarized by one feature such as the minimum voltage. For this, a more general feature extraction method such as principal component analysis (PCA) is typically used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b4795",
   "metadata": {},
   "source": [
    "## Notebook summary\n",
    "In this notebook, we've looked closely at just 100 seconds of one channel in a 384-channel recording. You can imagine how long spike sorting would take if we needed to do this for *all* of the data. Thankfully, there are more automated spikesorting tools which enable researchers to automatically sort their data, with just a little bit of sorting by hand. In the next notebook, we'll work with data that has already been sorted for us. The resulting data simply has spike times for each sorted neuron (or \"unit\") -- the moments in the experiment where a spike happened. This is the most common format for data sharing of extracellularly recorded data, since it's much more concise, and the hard work of spike sorting has already happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c963a8",
   "metadata": {},
   "source": [
    "## Notes & resources\n",
    "This notebook borrows inspiration and code from [this tutorial](https://github.com/multichannelsystems/McsPyDataTools/blob/master/McsPyDataTools/docs/McsPy-Tutorial_DataAnalysis.ipynb) which is protected under a copyright: Copyright (c) 2018, Multi Channel Systems MCS GmbH\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ae714",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}