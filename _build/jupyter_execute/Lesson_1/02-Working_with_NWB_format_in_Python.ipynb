{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with NWB in Python\n",
    "\n",
    "On the previous page, we demonstrated how to obtain a dataset with DANDI. Now that you have a dataset downloaded, let's take a closer look at what it contains.\n",
    "\n",
    "Working with our NWB file in Python requires **[PyNWB](https://pynwb.readthedocs.io/en/stable/index.html)**, a package specifically designed to work with NWB files.\n",
    "\n",
    "Below, we'll use the `NWBHDF5IO` class from this package, which will allow us to easily read NWB files.\n",
    "\n",
    "<mark>**Note**: Before running this notebook, please ensure that you have 1) set up your coding environment ([How to Use this Book](https://nwb4edu.github.io/Introduction/Using_This_Book.html)) and 2) completed the previous section to obtain the dataset we'll be interacting with below.\n",
    "\n",
    "## Step 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from the PyNWB package\n",
    "from pynwb import NWBHDF5IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Read the NWB file\n",
    "\n",
    "We can access the data in our NWB file in two steps:\n",
    "\n",
    "1. **Assign our file as an NWBHDF5IO object**: We will use the [`NWBHDF5IO` class](https://pynwb.readthedocs.io/en/stable/pynwb.html#pynwb.NWBHDF5IO) to create our `NWBHDF5IO` object and map our file to HDF5 format.\n",
    "2. **Read our file** using the `read()` method.\n",
    "\n",
    "> For more information on how to read NWB files, please visit the <a href = 'https://pynwb.readthedocs.io/en/latest/tutorials/general/plot_file.html#reading-an-nwb-file'> Reading an NWB file</a> section from the NWB Basics Tutorial.\n",
    "\n",
    "<mark>**Note**: Each dataset may contain multiple NWB files for different subjects and sessions for a given experiment. Make sure you specify the exact file path to the single NWB file you wish to read. Below, we'll give the filename for one .nwb file within the folder that you downloaded in the last chapter.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '000006/sub-anm369962/sub-anm369962_ses-20170310.nwb', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m000006/sub-anm369962/sub-anm369962_ses-20170310.nwb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# assign file as an NWBHDF5IO object\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m io \u001b[38;5;241m=\u001b[39m \u001b[43mNWBHDF5IO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# read the file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m nwb_file \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/utils.py:668\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    667\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/pynwb/__init__.py:300\u001b[0m, in \u001b[0;36mNWBHDF5IO.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_namespaces:\n\u001b[1;32m    299\u001b[0m     tm \u001b[38;5;241m=\u001b[39m get_type_map()\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_namespaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_region\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_region\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     manager \u001b[38;5;241m=\u001b[39m BuildManager(tm)\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# XXX: Leaving this here in case we want to revert to this strategy for\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m#      loading cached namespaces\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# ns_catalog = NamespaceCatalog(NWBGroupSpec, NWBDatasetSpec, NWBNamespace)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# tm.copy_mappers(get_type_map())\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/utils.py:668\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    667\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:185\u001b[0m, in \u001b[0;36mHDF5IO.load_namespaces\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load cached namespaces from a file.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03mIf `file` is not supplied, then an :py:class:`h5py.File` object will be opened for the given `path`, the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m:raises ValueError: if both `path` and `file` are supplied but `path` is not the same as the path of `file`.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m namespace_catalog, path, namespaces, file_obj, driver, aws_region \u001b[38;5;241m=\u001b[39m popargs(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace_catalog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespaces\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maws_region\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs)\n\u001b[0;32m--> 185\u001b[0m open_file_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__resolve_file_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_region\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_region\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# need to close the file object that we just opened\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m open_file_obj:\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/hdmf/backends/hdf5/h5tools.py:158\u001b[0m, in \u001b[0;36mHDF5IO.__resolve_file_obj\u001b[0;34m(cls, path, file_obj, driver, aws_region)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m aws_region \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m             file_kwargs\u001b[38;5;241m.\u001b[39mupdate(aws_region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbytes\u001b[39m(aws_region, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 158\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfile_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_obj\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/jb/lib/python3.11/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '000006/sub-anm369962/sub-anm369962_ses-20170310.nwb', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# set the filename\n",
    "filename = '000006/sub-anm369962/sub-anm369962_ses-20170310.nwb'\n",
    "\n",
    "# assign file as an NWBHDF5IO object\n",
    "io = NWBHDF5IO(filename, 'r')\n",
    "\n",
    "# read the file\n",
    "nwb_file = io.read()\n",
    "\n",
    "nwb_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Task</b>: Look through the file above by clicking on the sideways black triangles to drop down different levels of the file structure. What's there that might be interesting to analyze?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Access Information within the NWB File Hierarchy\n",
    "\n",
    "One of the first steps when working with a new dataset is to figure out what is in the dataset, and where. Each NWB file is composed of various groups, which either contain attributes of our file (**metadata**) or the data itself.\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Metadata</b> is a common term to describe all of the information about an experiment. This could include everything from when the experiment was conducted, the ID of the subject (animal, human, goblin, etc.), the equipment details, etc. In essence, the metadata provides the context of the experiment. This is one of the first things you should review when youâ€™re encountering a new dataset.</div>\n",
    "\n",
    "Here is the structure of a typical NWB file:\n",
    "\n",
    "![NWB_file_structure.png](NWB_file_structure.png)\n",
    "\n",
    "In order to see which groups are in our file, we can use the `fields` attribute to return a dictionary containing the Groups of our NWB file. The dictionary **keys** are the various groups within the file which we will use to access the data we're ultimately interested in.\n",
    "\n",
    "> Need a refresher on dictionaries? Consider working through the free [Codecademy Python 3 lesson](https://www.codecademy.com/enrolled/courses/learn-python-3), or check the other resources on the [Data Science in Python](https://nwb4edu.github.io/Data_Science_In_Python/Introduction.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Groups for the nwb file \n",
    "nwb_fields = nwb_file.fields\n",
    "print(nwb_fields.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Metadata\n",
    "\n",
    "Let's first pull out some metadata for the experiment we downloaded.\n",
    "\n",
    "If you wish to access the related publications of the experimental data that you just downloaded, you can do so by accessing the `related_publications` attribute of your NWB file object. Plug in the \"doi:\" address that prints below into a browser window to check out the original publication describing this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the related publication\n",
    "nwb_file.related_publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each NWB file will also have information on where the experiment was conducted, which lab conducted the experiment, as well as a description of the experiment. This information can be accessed using `institution`, `lab`, and `experiment_description`, attributes on our nwb_file, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get metadata from NWB file \n",
    "print('The experiment within this NWB file was conducted at',nwb_file.institution,'.'\\\n",
    "      ,nwb_file.experiment_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed at this point, we can access datasets from each group in our nwb_file with the following syntax: `nwb_file.GROUPNAME`, just as we would typically access an attribute of object in Python. Below we will demonstrate some of the most useful groups within an NWB object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition \n",
    "\n",
    "The `acquisition` group contains datasets of acquisition data, mainly `TimeSeries` objects belonging to this NWBFile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_file.acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, the acquisition group contains one dataset, `lick_times`. This dataset has one field, `time_series`, which contains two time series objects, `lick_left_times` and `lick_right_times`. To access the actual data arrays of these objects we must first subset our dataset of interest from the group. We can then use `timestamps[:]` to return a list of timestamps for when the animal licked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select our dataset of interest \n",
    "dataset = 'lick_times'\n",
    "field = 'lick_right_times'\n",
    "\n",
    "lick_r_dataset = nwb_file.acquisition[dataset][field]\n",
    "\n",
    "# return the first 10 values in data array \n",
    "lick_r_data_array = lick_r_dataset.timestamps[:10][:10]\n",
    "\n",
    "print(lick_r_data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervals \n",
    "\n",
    "The `intervals` group contains all time interval tables from the experiment -- things like, did the animal respond on the behavioral trial? Usefully, we can take `intervals` and convert it to a tidy dataframe using `to_dataframe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the group of interest from the nwb file \n",
    "intervals = nwb_file.intervals\n",
    "\n",
    "# Pull out trials and assign it as a dataframe\n",
    "interval_trials_df = intervals['trials'].to_dataframe()\n",
    "interval_trials_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you're wondering what these columns are, the `description` attribute provides a short description on each column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the description of each col in our dataframe\n",
    "for col in interval_trials_df:\n",
    "    print(col,':',intervals['trials'][col].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Units\n",
    "\n",
    "But wait, where's all of the neural data? The `units` group in our NWB file contains the processed signals from our individual neurons (**units**), including information about the spike sorting quality as well as the spike times -- when each of these cells fired an action potential. Much like the `intervals` group, `units` can also be assigned to a dataframe.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Why \"units\"?</b> In extracellular electrophysiology, we aren't recording *directly* from neurons. Instead, we're recording from the space around many neurons. As a result, researchers need to take the recorded voltage streams and determine which spikes in voltage originated in different neurons. This process is called <b>spike sorting</b> (discussed in detail in a future lesson!). Although we can do spike sorting fairly automatically and be fairly confident that we've correctly identified different neurons, we can't know *with complete confidence*. So, researchers tend to call \"neurons\" in extracellular recordings \"units,\" reflecting that we *think* it's a separate neuron, but don't know for sure. You'll also see \"multi-unit activity\" (MUA) in some papers, in which case the researchers were unable to separate single neurons.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "units = nwb_file.units\n",
    "units_df = units.to_dataframe()\n",
    "units_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to know where these spikes are coming from, we can look at the `electrodes` attribute. The `electrodes` group contains metadata about the electrodes used in the experiment, including the location of the electrodes, the type of filtering done on that channel, and which electrode group the electrode belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# electrode positions \n",
    "electrodes = nwb_file.electrodes\n",
    "electrodes_df = electrodes.to_dataframe()\n",
    "electrodes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wondering what something in this table is? We can once again dig out the descriptions:\n",
    "\n",
    "> Not sure what's happening below? Consider working through the [Codecademy Python 3 course](https://www.codecademy.com/enrolled/courses/learn-python-3) for a refresher on for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the description of each col in our dataframe\n",
    "for col in electrodes_df:\n",
    "    print(col,':',nwb_file.electrodes[col].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what this file contains, we can finally take a look at some of the data! We'll do that in the next section. ðŸ’ƒ\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Additional Resources \n",
    "* For a detailed explanation of all groups contained within an NWB File object please visit the <a href = 'https://pynwb.readthedocs.io/en/stable/pynwb.file.html'>pynwb.file.NWBFile</a> section of the PyNWB documentation. \n",
    "* The [OpenScope DataBook](https://alleninstitute.github.io/openscope_databook/basics/read_nwb.html) also contains explanations of what is contained within NWB files.\n",
    "* Accessing metadata for different kinds of NWB files can be tricky. [Here are some useful helper scripts](https://alleninstitute.github.io/openscope_databook/basics/get_dandiset_metadata.html) from the OpenScope DataBook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}