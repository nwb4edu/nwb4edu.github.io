{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1655aa86",
   "metadata": {},
   "source": [
    "# Spike Sorting\n",
    "\n",
    "When you record extracellular electrophysiology data, the first step is figuring out which action potentials (or \"spikes\") came from which neurons. The process of doing this is called **spike sorting**.\n",
    "\n",
    "Below, we'll work with [this dataset](https://dandiarchive.org/dandiset/000053/0.210819.0345) from [Lisa Giocomo's lab at Stanford](https://giocomolab.weebly.com/) to demonstrate the simplest form of spike sorting: thresholding, followed by feature detection.\n",
    "\n",
    "<mark>**Note**:  The code below requires a different dataset than the one we interacted with in the last chapter. Because this dataset contains all of the raw recording data, it is much, much bigger. As a result, the best way to work with it is through the Dandihub. Check out the [Setup page](https://nwb4edu.github.io/Chapter_01/Setup.html) for instructions on how to run this on the Dandihub.</mark>\n",
    "\n",
    "First, we need to find the correct URL for the dataset on the NWB's Amazon S3 storage system. There is a tool to do so within the dandiapi, which we'll use below to get the URL for one session within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a37ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "dandiset_id = '000053' # giocomo data\n",
    "filepath = 'sub-npI5/sub-npI5_ses-20190414_behavior+ecephys.nwb' \n",
    "\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(filepath)\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "    \n",
    "print(s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f0e81",
   "metadata": {},
   "source": [
    "Now, we can read this path, but we'll stream it, rather than downloading it! Below, we'll print some useful information about this experiment. We will also access a dataset we haven't interacted with yet: \"ElectricalSeries\". As the name suggests, this contains raw electrophysiology data -- exactly what we need to sort! We will assign a portion of this to an object called `ephys_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3e12e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "with NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3') as io:\n",
    "    nwbfile = io.read()\n",
    "    print(nwb_file.experiment_description)\n",
    "    print(nwbfile.acquisition['ElectricalSeries'].data.shape)\n",
    "    sampling_freq = nwbfile.acquisition['ElectricalSeries'].resolution # get the sampling frequency in Hz\n",
    "    ephys_data = (nwbfile.acquisition['ElectricalSeries'].data[:3000000, 99])*nwbfile.acquisition['ElectricalSeries'].conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe86543",
   "metadata": {},
   "source": [
    "Before we dive into spike sorting, let's take a look at the data. Below, we'll import a couple additional packages, generate a list of timestamps, and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate a vector of timestamps\n",
    "timestamps = np.arange(start=0,stop=100,1/sampling_frequency)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,3))\n",
    "plt.plot(timestamps,ephys_data)\n",
    "plt.ylabel('Voltage (V)')\n",
    "plt.xlabel('Seconds (s)')\n",
    "#plt.xlim(1.053,1.055)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5f787",
   "metadata": {},
   "source": [
    "In the data above,there are clear places where the data \"spikes\". These are extracellularly recorded action potentials!\n",
    "\n",
    "One of the most straightforward ways to spike sort is to simply detect these using a threshold. Whenever the signal passes this threshold, we'll clip that out. We can determine a reasonable threshold using one rule of thumb: when the signal is 5 times the noise MAD, that's enough signal to noise that it's likely to be an action potential. We'll calculate that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = np.std(ephys_data)\n",
    "noise_mad = np.median(np.absolute(ephys_data)) / 0.6745  # WHY .6745??\n",
    "recommended_threshold = -5 * noise_mad\n",
    "print('Noise Estimate by Standard Deviation: ', noise_std)\n",
    "print('Noise Estimate by MAD Estimator     : ', noise_mad)\n",
    "print('Recommended Spike Threshold         : ', recommended_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87219f13",
   "metadata": {},
   "source": [
    "The next few steps require more functions than we need to look at in detail. Below, we'll import those functions by running a script to create them. We'll then use the `%whos` magic command to see the functions we imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91292ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                     Type               Data/Info\n",
      "---------------------------------------------------------\n",
      "DandiAPIClient               type               <class 'dandi.dandiapi.DandiAPIClient'>\n",
      "NWBHDF5IO                    ABCMeta            <class 'pynwb.NWBHDF5IO'>\n",
      "align_to_minimum             function           <function align_to_minimum at 0x7ffb346af2f0>\n",
      "asset                        RemoteBlobAsset    DANDI:assets/2d8d2eb4-dff2-4bfd-998c-5d6dc1c5a611\n",
      "client                       DandiAPIClient     <dandi.dandiapi.DandiAPIC<...>object at 0x7ffb304ab588>\n",
      "dandiset_id                  str                000053\n",
      "detect_threshold_crossings   function           <function detect_threshol<...>ssings at 0x7ffb3058b510>\n",
      "extract_waveforms            function           <function extract_waveforms at 0x7ffb346af378>\n",
      "filepath                     str                sub-npI5/sub-npI5_ses-201<...>0414_behavior+ecephys.nwb\n",
      "get_next_minimum             function           <function get_next_minimum at 0x7ffb346af268>\n",
      "np                           module             <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "plot_waveforms               function           <function plot_waveforms at 0x7ffb346af400>\n",
      "plt                          module             <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "s3_path                      str                https://dandiarchive.s3.a<...>3a-42a7-91e8-48e9f59c2881\n"
     ]
    }
   ],
   "source": [
    "%run spikesorting_helperfunctions.py\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d7a04",
   "metadata": {},
   "source": [
    "Now, we can use those functions to detect threshold crossings. Below, we'll use two separate functions `detect_threshold_crossings` and `align_to_minimum` to find these spikes. We'll then plot our original signal with the detected spikes marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7925522",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossings = detect_threshold_crossings(ephys_data, sampling_freq, recommended_threshold, 0.003) # dead time of 3 ms\n",
    "spks = align_to_minimum(ephys_data, sampling_freq, crossings, 0.002) # search range 2 ms\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(timestamps,ephys_data)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Voltage (uV)')\n",
    "plt.plot(spks,[recommended_threshold]*spks.shape[0], 'ro', ms=2)\n",
    "plt.xlim([0,len(ephys_data)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bdc57",
   "metadata": {},
   "source": [
    "Finally, we can cut out the waveforms from these detected spikes so that we can look at their shape. The location (the channel on the electrode shank) and shape of a waveform is one of the main pieces of evidence to show that the waveform was recorded from a cell. This will help us determine whether or not there is just one or more neurons recorded on this channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = 0.001 # 1 ms\n",
    "post= 0.002 # 2 ms\n",
    "waveforms = extract_waveforms(ephys_data, sampling_freq, spks, pre, post)\n",
    "plot_waveforms(waveforms, sampling_freq, pre, post, n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db614c46",
   "metadata": {},
   "source": [
    "Looking at the plot above, would you say there is just one waveform here, or multiple?\n",
    "\n",
    "Indeed, it looks like there might be two -- one that is very high amplitude, and another that is lower amplitude. We can use **feature detection** to investigate our waveforms. Below, we'll plot the minimum and maximum voltages in the recorded waveforms to see if there are distinct clusters of waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_voltage = np.amin(waveforms, axis=1)\n",
    "max_voltage = np.amax(waveforms, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(min_voltage, max_voltage,'.')\n",
    "plt.xlabel('Minimum Voltage (V)')\n",
    "plt.ylabel('Maximum Voltage (V)')\n",
    "plt.title('Min/Max Spike Voltages')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f63dab",
   "metadata": {},
   "source": [
    "Looking at the scatterplot above, would you say there is more than one cluster of waveforms?\n",
    "\n",
    "### Notebook summary\n",
    "In this notebook, we've looked closely at just 100 seconds of one channel in a 384-channel recording. You can imagine how long spike sorting would take if we needed to do this for *all* of the data. Thankfully, there are more automated spikesorting tools which enable researchers to automatically sort their data, with just a little bit of sorting by hand. In the next notebook, we'll work with data that has already been sorted for us. The resulting data simply has spike times for each sorted neuron (or \"unit\") -- the moments in the experiment where a spike happened. This is the most common format for data sharing of extracellularly recorded data, since it's much more concise, and the hard work of spike sorting has already happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c963a8",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "https://pynwb.readthedocs.io/en/stable/pynwb.ecephys.html\n",
    "\n",
    "https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
