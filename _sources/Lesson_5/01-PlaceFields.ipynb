{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6746672a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# How does the brain encode space?\n",
    "\n",
    "In 2006, a group of researchers published a landmark paper ([Sargolini et al. 2006](https://doi.org/10.1126/science.1125572)) demonstrating that cells in the hippocampus fired in regular spatial intervals. These researchers, May-Britt & Edvard Moser, were [awarded the Nobel Prize](https://www.nobelprize.org/prizes/medicine/2014/press-release/) in 2014 for their efforts.  \n",
    "\n",
    "This tutorial demonstrates how to access the dataset -- the very one they won the Nobel Prize for! -- published in using `dandi`.\n",
    "\n",
    "The [dataset](https://dandiarchive.org/dandiset/000582/draft) contains spike times for recorded grid cells from the medial entorhinal cortex (MEC) in rats that explored two-dimensional environments. The behavioral data includes position from the tracking LED(s).\n",
    "\n",
    "#### Contents:\n",
    "\n",
    "* [Streaming NWB files](#stream-nwb)\n",
    "* [Accessing data and metadata](#access-nwb)\n",
    "* [Accessing behavior data](#position)\n",
    "* [Accessing spike times](#spike-times)\n",
    "* [Showing rate maps](#rate-maps)\n",
    "\n",
    "#### Authors\n",
    "This notebook was authored by Dorota Jareka, Szonja Weigl, and Ben Dichter. It was edited by Ashley Juavinett.\n",
    "\n",
    "<mark>**Note #1**: The best way to complete this notebook is through the Dandihub. If you're not already running this book on the Dandihub, read [Using this Book](https://nwb4edu.github.io/Chapter_01/Using_This_Book.html) for instructions on how to run this on the Dandihub.</mark>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce9ed7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Streaming NWB files <a class=\"anchor\" id=\"stream-nwb\"></a>\n",
    "\n",
    "This section demonstrates how to access the files on DANDI without downloading them. If you need a refresher, we discussed this in [Lesson 1](https://nwb4edu.github.io/Lesson_1/01-Obtaining_Datasets_with_DANDI.html). You can also reference the [Streaming NWB files](https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/streaming.html) tutorial from `PyNWB`.\n",
    "\n",
    "The `DandiAPIClient` can be used to get the S3 URL of [this NWB file](https://dandiarchive.org/dandiset/000582/draft) stored in the DANDI Archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528383c1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "dandiset_id, nwbfile_path = \"000582\", \"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\" # file size ~15.6MB\n",
    "\n",
    "# Get the location of the file on DANDI\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, 'draft').get_asset_by_path(nwbfile_path)\n",
    "    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "    \n",
    "print(s3_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3baab9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a virtual filesystem using `fsspec` which will take care of requesting data from the S3 bucket whenever data is read from the virtual file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25589af3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "from fsspec import filesystem\n",
    "from h5py import File\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# first, create a virtual filesystem based on the http protocol\n",
    "fs=filesystem(\"http\")\n",
    "\n",
    "# create a cache to save downloaded data to disk (optional)\n",
    "fs = CachingFileSystem(\n",
    "    fs=fs,\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "file_system = fs.open(s3_url, \"rb\")\n",
    "file = File(file_system, mode=\"r\")\n",
    "# Open the file with NWBHDF5IO\n",
    "io = NWBHDF5IO(file=file, load_namespaces=True)\n",
    "\n",
    "nwbfile = io.read()\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db38c3d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Access metadata <a class=\"anchor\" id=\"access-nwb\"></a>\n",
    "\n",
    "First, let's take a look at the metadata in this file.\n",
    "\n",
    "`subject` is an attribute of the `nwbfile`. It holds information about the experimental subject, such as age (in [ISO 8601 Duration format](https://en.wikipedia.org/wiki/ISO_8601#Durations)), sex, and species in latin binomial nomenclature.\n",
    "\n",
    "> **Tasks**:\n",
    "> * Inspect the subject field. If you need a reminder for how to do this, see [Step 3 in Lesson 1](https://nwb4edu.github.io/Lesson_1/02-Working_with_NWB_format_in_Python.html).\n",
    "> * Take a look at other attributes of `nwbfile` as well. Hint: you can hit 'tab' after `nwbfile.` to see all of the attributes and methods of the `nwbfile` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb1664",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at subject here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d607786",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Accessing behavior data <a class=\"anchor\" id=\"position\"></a>\n",
    "\n",
    "The \"behavior\" processing module holds the behavior data in the NWB file which can be accessed as\n",
    "`nwbfile.processing[\"behavior\"]`.\n",
    "\n",
    "### Position\n",
    "\"Position\" gives us the location of the mouse in space. The position data is stored in a `SpatialSeries` object which can be accessed from the \"Position\" container as `nwbfile.processing[\"behavior\"][\"Position\"]`.\n",
    "\n",
    "> **Task**: Look at the original paper. How did the researchers figure out the position of the animal?\n",
    "\n",
    "Note that not all sessions have position data from two tracking LEDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0b29a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatial_series = nwbfile.processing[\"behavior\"][\"Position\"][\"SpatialSeriesLED1\"]\n",
    "\n",
    "# Inspect conversion and data\n",
    "print(spatial_series.conversion)\n",
    "print(spatial_series.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1637b",
   "metadata": {},
   "source": [
    "Now that we have the behavioral data, we can plot it. The `conversion` field tells us how to translate the values in `data` to meters. The `data` object here has 3,000 entries for x positions (at index 0) and y positions (at index 1). So, the first thing we'll do is convert the data into meters, and then we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9158a6c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract & convert x and y positions\n",
    "x =spatial_series.data[:, 0] * spatial_series.conversion\n",
    "y =spatial_series.data[:, 1] * spatial_series.conversion\n",
    "\n",
    "# Plot and label!\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('X (meters)')\n",
    "plt.ylabel('Y (meters)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f4e66",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Accessing spike times <a class=\"anchor\" id=\"spike-times\"></a>\n",
    "\n",
    "As a reminder, the `Units` table holds the spike times which can be accessed as `nwbfile.units` and can also be converted to a pandas `DataFrame`.\n",
    "\n",
    "> **Task**: Access `units` below and convert it to a dataframe. Assign this to `units_df`. If you need a reminder for how to do this, refer back to Lesson 1. Inspect the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3560b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nwbfile.units.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad173ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(Site note: for an interactive visualization of spike times and position, try out [Neurosift](https://flatironinstitute.github.io/neurosift/?p=/nwb&url=https://dandiarchive.s3.amazonaws.com/blobs/ec1/842/ec1842a0-2229-4096-8dcd-d42b49f9dd49)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d80db",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualizing rate maps <a class=\"anchor\" id=\"rate-maps\"></a>\n",
    "\n",
    "As you can see in the dataframe above, there are 8 recorded neurons (indices 0 through 7) in this dataset.  This section demonstrates how to show the rate maps of those recorded cells. We will use [PYthon Neural Analysis Package](https://pynapple-org.github.io/pynapple/) (`pynapple`) to calculate the rate maps. The first cell will install `pynapple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c5af70",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynapple\n",
      "  Downloading pynapple-0.2.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (1.3.5)\n",
      "Requirement already satisfied: numba in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (0.43.1)\n",
      "Requirement already satisfied: numpy in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (1.20.3)\n",
      "Requirement already satisfied: scipy in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (1.7.3)\n",
      "Requirement already satisfied: pynwb in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (2.3.3)\n",
      "Requirement already satisfied: tabulate in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (0.8.9)\n",
      "Requirement already satisfied: pyqt5 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (5.15.9)\n",
      "Collecting pyqtgraph (from pynapple)\n",
      "  Downloading pyqtgraph-0.12.4-py3-none-any.whl (995 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m995.8/995.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynapple) (3.7.0)\n",
      "Requirement already satisfied: llvmlite>=0.28.0dev0 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from numba->pynapple) (0.28.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pandas->pynapple) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pandas->pynapple) (2018.9)\n",
      "Requirement already satisfied: hdmf>=3.5.4 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynwb->pynapple) (3.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pynwb->pynapple) (57.0.0)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.11 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pyqt5->pynapple) (12.12.1)\n",
      "Requirement already satisfied: PyQt5-Qt5>=5.15.2 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from pyqt5->pynapple) (5.15.2)\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from hdmf>=3.5.4->pynwb->pynapple) (3.0.1)\n",
      "Collecting ruamel.yaml>=0.16 (from hdmf>=3.5.4->pynwb->pynapple)\n",
      "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: importlib-metadata<4.3 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from hdmf>=3.5.4->pynwb->pynapple) (4.2.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ashley/anaconda3/lib/python3.7/site-packages (from hdmf>=3.5.4->pynwb->pynapple) (5.1.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->pynapple) (1.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from importlib-metadata<4.3->hdmf>=3.5.4->pynwb->pynapple) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from importlib-metadata<4.3->hdmf>=3.5.4->pynwb->pynapple) (4.5.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6.0->hdmf>=3.5.4->pynwb->pynapple) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6.0->hdmf>=3.5.4->pynwb->pynapple) (0.14.11)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/ashley/anaconda3/lib/python3.7/site-packages (from ruamel.yaml>=0.16->hdmf>=3.5.4->pynwb->pynapple) (0.2.7)\n",
      "Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ruamel.yaml, pyqtgraph, pynapple\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel.yaml 0.17.32\n",
      "    Uninstalling ruamel.yaml-0.17.32:\n",
      "      Successfully uninstalled ruamel.yaml-0.17.32\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deeplabcut 2.2.3 requires numba>=0.54, but you have numba 0.43.1 which is incompatible.\n",
      "deeplabcut 2.2.3 requires scikit-image>=0.17, but you have scikit-image 0.14.2 which is incompatible.\n",
      "deeplabcut 2.2.3 requires tables>=3.7.0, but you have tables 3.6.1 which is incompatible.\n",
      "sleap 1.2.9 requires attrs<=21.4.0,>=21.2.0, but you have attrs 19.1.0 which is incompatible.\n",
      "sleap 1.2.9 requires imageio<=2.15.0, but you have imageio 2.25.1 which is incompatible.\n",
      "sleap 1.2.9 requires tensorflow<2.9.0,>=2.6.3; platform_machine != \"arm64\", but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pynapple-0.2.4 pyqtgraph-0.12.4 ruamel.yaml-0.18.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pynapple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42a2d1",
   "metadata": {},
   "source": [
    "Using the `compute_2d_tuning_curves()` function from `pynapple` (imported above as `nap`), we can compute firing rate as a function of position (map of neural activity as the animal explored the environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08711569",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pynapple as nap\n",
    "\n",
    "# Compute position over time\n",
    "position_over_time = nap.TsdFrame(\n",
    "    d=spatial_series.data[:],\n",
    "    t=spatial_series.timestamps[:],\n",
    "    columns=[\"x\",\"y\"],\n",
    ")\n",
    "\n",
    "spike_times_group = nap.TsGroup({cell_id: nap.Ts(spikes) for cell_id, spikes in enumerate(nwbfile.units[\"spike_times\"])})\n",
    "\n",
    "num_bins = 15\n",
    "\n",
    "rate_maps, position_bins = nap.compute_2d_tuning_curves(\n",
    "    spike_times_group,\n",
    "    position_over_time,\n",
    "    num_bins,\n",
    ")\n",
    "\n",
    "print(type(rate_maps))\n",
    "print(len(rate_maps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2ee94",
   "metadata": {},
   "source": [
    "The `rate_maps` object generated above is a dictionary, in which each entry's key is the unit ID, and the value is the rate map.\n",
    "\n",
    "> **Task**: Using `plt.imshow()`, look at a few rate maps! As a reminder, you can extract the value of a dictionary using the syntax `dictionary_name[key]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bbb7ec-54c7-4405-af14-4a84f98039b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "\n",
    "# Plot a rate map or two here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab0bd8-8b6f-4da3-9e32-97eeaf1a0648",
   "metadata": {},
   "source": [
    "## Visualizing grid cells activity\n",
    "\n",
    "To determine whether the firing fields of individual cells formed a grid structure, we will calculate the spatial autocorrelation for the rate map of each cell.\n",
    "\n",
    "The autocorrelograms are based on Pearson’s product moment correlation coefficient with\n",
    "corrections for edge effects and unvisited locations.  With λ (x, y) denoting the average rate of a cell at location (x, y), the autocorrelation between the fields with spatial lags of τx and τy was estimated as:\n",
    "\n",
    "<img src=\"autocorrelation_equation.png\" alt=\"autocorrelation_equation\" />\n",
    "\n",
    "where the summation is over all n pixels in λ (x, y) for which rate was estimated for both λ (x, y) and λ (x - τx, y - τy). Autocorrelations were not estimated for lags of τx, τy where n < 20.\n",
    "\n",
    "The degree of spatial periodicity (gridness) can be determined for each cell by rotating the autocorrelation map for each cell in steps of 6 degrees (from 0 to 180 degrees) and computing the correlation between the rotated map and the original. The correlation is confined to the area defined by a circle around the peaks that are closest to the centre of the map, and the central peak is not included in the analysis.\n",
    "\n",
    "The ‘gridness’ of a cell can be expressed as the difference between the lowest correlation at 60 and 120 degrees (where a peak correlation would be expected due to the triangular nature of the grid) and the highest correlation at 30, 90, and 150 degrees (where the minimum correlation would be expected). When the correlations at 60 and 120 degrees of rotation exceeded each of the correlations at 30, 90 and 150 degrees (gridness > 0), the cell was classified as a grid cell.\n",
    "\n",
    "First, let's define our functions to help us make these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62c0f1-b57c-44e7-99f1-20c35d11b8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_coer_arr(arr, rad_min=None, rad_max=None):\n",
    "    \"\"\" Creating an array for correlation(tau_x, tau_y)\n",
    "    Takes tau_x from the range (-arr.shape[0]+1, arr.shape[0]-1) and the same for tau_y\n",
    "    \"\"\"\n",
    "    sh_x, sh_y = arr.shape\n",
    "    # creating an array full of nan's\n",
    "    coer_arr = np.full((2*sh_x-1, 2*sh_y-1), np.nan)\n",
    "    for ii in range(0, 2*(sh_x-1)):\n",
    "        for jj in range(0, 2*(sh_y-1)):\n",
    "            # shifting tau_x/y\n",
    "            tau_x = ii-sh_x+1\n",
    "            tau_y = jj-sh_y+1\n",
    "            # if rad_max and rad_min is provided, I only calculate the correlation for points between rad_min and rad_max\n",
    "            if rad_max is not None and ((tau_x**2 + tau_y**2)**0.5 > rad_max):\n",
    "                continue\n",
    "            if rad_min is not None and ((tau_x**2 + tau_y**2)**0.5 < rad_min):\n",
    "                continue\n",
    "            coer_arr[ii, jj] = pearson_autocor(arr, lag_x=tau_x, lag_y=tau_y)\n",
    "    return coer_arr\n",
    "\n",
    "def pearson_autocor(arr, lag_x, lag_y):\n",
    "    \"\"\" Calculates Pearson autocorrelation for an array that can contain NaN values.\"\"\"\n",
    "    sh_x, sh_y = arr.shape\n",
    "    if abs(lag_x) >= sh_x or abs(lag_y) >= sh_y:\n",
    "        raise Exception(f\"abs(lag_x), abs(lag_y) have to be smaller than {sh_x}, {sh_y}, but {lag_x}, {lag_y} provided\")\n",
    "\n",
    "    # calculating sum for elements that meet the requirements\n",
    "    n = 0\n",
    "    sum1, sum2, sum3, sum4, sum5 = 0, 0, 0, 0, 0\n",
    "    for ii in range(0, sh_x):\n",
    "        for jj in range(0, sh_y):\n",
    "            # checking if the indices are withing the array\n",
    "            if 0 <= ii-lag_x < sh_x and 0 <= jj-lag_y < sh_y:\n",
    "                # checking if both values (in ii,jj and shifted) are not nan\n",
    "                if not np.isnan(arr[ii, jj]) and not np.isnan(arr[ii-lag_x, jj-lag_y]):\n",
    "                    \n",
    "                    n += 1\n",
    "                    sum1 += arr[ii, jj] * arr[ii-lag_x, jj-lag_y]\n",
    "                    sum2 += arr[ii, jj] \n",
    "                    sum3 += arr[ii-lag_x, jj-lag_y]\n",
    "        \n",
    "                    sum4 += (arr[ii, jj])**2\n",
    "                    sum5 += (arr[ii-lag_x, jj-lag_y])**2\n",
    "\n",
    "              \n",
    "    # according to the paper they had this limit for number of points\n",
    "    if n < 20:\n",
    "        return np.nan\n",
    "\n",
    "    numerator = n * sum1 - sum2 * sum3\n",
    "    denominator = (n * sum4 - sum2**2)**0.5 * (n * sum5 - sum3**2)**0.5\n",
    "    cor = numerator / denominator\n",
    "\n",
    "    return cor\n",
    "\n",
    "def pearson_cor_2arr(arr1, arr2):\n",
    "    \"\"\" Calculates Pearson correlation for two arrays with the same shape and no NaN.\"\"\"\n",
    "    if not arr1.shape == arr2.shape:\n",
    "        raise Exception(\"Both arrays should have the same shape\")\n",
    "    n = arr1.shape[0] * arr1.shape[1]\n",
    " \n",
    "    numerator = n * np.sum(arr1 * arr2) - np.sum(arr1) * np.sum(arr2)\n",
    "    denominator = (n * np.sum(arr1**2) - np.sum(arr1)**2)**0.5 * (n * np.sum(arr2**2) - np.sum(arr2)**2)**0.5\n",
    "    return numerator / denominator\n",
    "\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b27b1",
   "metadata": {},
   "source": [
    "Now that we have these functions, let's use them to analyze the data. There's *a lot* of code below and it uses a new package `plotly` -- don't worry about the code, just the figures it creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb17ed7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from scipy.ndimage import rotate\n",
    "import scipy\n",
    "\n",
    "rate_maps_50_bin, _ = nap.compute_2d_tuning_curves(\n",
    "    spike_times_group,\n",
    "    position_over_time,\n",
    "    50,\n",
    ")\n",
    "\n",
    "for cell_ind in range(len(rate_maps)):\n",
    "    unit_name = nwbfile.units[\"unit_name\"][cell_ind]\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=3,\n",
    "        subplot_titles=(f'{unit_name} rate map', f'{unit_name} auto-correlation', \"periodicity\"),\n",
    "    )\n",
    "\n",
    "    rate_map_plot = go.Heatmap(z=rate_maps[cell_ind], colorscale='Viridis', showscale=False)\n",
    "    fig.add_trace(rate_map_plot, row=1, col=1)\n",
    "\n",
    "    # Compute auto-correlation\n",
    "    autocorr = create_coer_arr(rate_maps_50_bin[cell_ind], rad_max=34, rad_min=6)\n",
    "    autocorr_nonan = np.nan_to_num(autocorr, copy=True, nan=0.0)\n",
    "\n",
    "    correlations = []\n",
    "    angles = np.arange(0, 186, 6)\n",
    "    for angle in angles:\n",
    "        autocorr_nonan_rotated = rotate(autocorr_nonan, angle=angle, reshape=False)\n",
    "        cor = pearson_cor_2arr(autocorr_nonan_rotated, autocorr_nonan)\n",
    "        correlations.append(cor)\n",
    "    \n",
    "    gridness = max(correlations[10], correlations[20]) - max(correlations[5], correlations[15], correlations[25])\n",
    "    gridness = np.round(gridness, 2)\n",
    "    \n",
    "    autocorr_rate_map = go.Heatmap(z=autocorr, colorscale='Viridis', showscale=False)\n",
    "    fig.add_trace(autocorr_rate_map, row=1, col=2)\n",
    "    \n",
    "    line_trace = go.Scatter(\n",
    "        x=angles,\n",
    "        y=correlations,\n",
    "        mode='lines',\n",
    "        marker=dict(color=\"black\"),\n",
    "        \n",
    "    )\n",
    "    \n",
    "    fig.add_trace(line_trace, row=1, col=3)\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    \n",
    "    fig.update_xaxes(showticklabels=True, row=1, col=3, title_text=\"Rotation (deg)\")\n",
    "    fig.update_yaxes(showticklabels=True, row=1, col=3, title_text=\"Correlation (r)\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Gridness score {gridness}\",\n",
    "        xaxis3 = dict(\n",
    "            tickmode=\"array\",\n",
    "            tickvals=[30, 60, 90, 120, 150, 180],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7669f-6c0d-4831-b8d4-a3c811252796",
   "metadata": {},
   "source": [
    "As you can see most cells could be classified as \"grid cells\", the auto-correlation maps look good and they have high periodicity, i.e., there is clear sinusoidal behavior of the periodicity function with clear maximums around 0, 60, 120 and 180 deg. However, one example (t3c4) doesn't have clear autocorrelation and the periodicity function has no sinusoidal behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
